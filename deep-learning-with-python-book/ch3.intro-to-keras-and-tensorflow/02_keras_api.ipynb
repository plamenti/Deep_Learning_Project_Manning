{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02.keras_api.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3_7q4s7wXdg"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras import models"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDCPYHoqwpLr"
      },
      "source": [
        "### Layers: the building blocks of deep learning\r\n",
        "\r\n",
        "The fundamental data structure in neural networks is the layer. A layer is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. Some layers are stateless, but more frequently layers have a state: the layer’s weights, one or several tensors learned with stochastic gradient descent, which together contain the network’s knowledge.\r\n",
        "\r\n",
        "Different types of layers are appropriate for different tensor formats and different types of data processing. For instance, simple vector data, stored in 2D tensors of shape (samples, features), is often processed by densely connected layers, also called fully connected or dense layers (the Dense class in Keras). Sequence data, stored in 3D tensors of shape (samples, timesteps, features), is typically processed by recurrent layers, such as an LSTM layer, or 1D convolution layers (Conv1D). Image data, stored in 4D tensors, is usually processed by 2D convolution layers (Conv2D).\r\n",
        "You can think of layers as the LEGO bricks of deep learning, a metaphor that is made explicit by Keras. Building deep-learning models in Keras is done by clipping together compatible layers to form useful data-transformation pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvxcma6FyF_A"
      },
      "source": [
        "#### The base **Layer** class in Keras\r\n",
        "\r\n",
        "A simple API should have a single abstraction around which everything is centered. In Keras, that’s the Layer class. Everything in Keras is either a Layer or something that closely interacts with a **Layer**.\r\n",
        "\r\n",
        "A **Layer** is an object that encapsulates some state (**weights**) and some computation (**a forward pass**). The **weights** are typically defined in a **build()** (although they could also be created in the constructor **init()**), and the computation is defined in the **call()** method.\r\n",
        "\r\n",
        "In the previous chapter, we implemented a NaiveDense class that contained two weights W and b and applied the computation output = activation(dot(W, input) + b). This is what the same layer would look like in Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKcPLMi_wdOw"
      },
      "source": [
        "class SimpleDense(keras.layers.Layer):  # All Keras layers inherit from the base Layer class.\r\n",
        "  def __init__(self, units, activation=None):\r\n",
        "    super(SimpleDense, self).__init__()\r\n",
        "    self.units = units\r\n",
        "    self.activation = activation\r\n",
        "\r\n",
        "  def build(self, input_shape): # Weight creation takes place in the build() method.\r\n",
        "    input_dim = input_shape[-1]\r\n",
        "    self.W = self.add_weight(shape=(input_dim, self.units), initializer=\"random_normal\")  # add_weight is a shortcut method for creating weights. It is also possible to create standalone variables and assign them as layer attributes,\r\n",
        "                                                                                          #  like: self.W = tf.Variable(tf.random.uniform(w_shape)).\r\n",
        "    self.b = self.add_weight(shape=(self.units), initializer=\"zeros\")\r\n",
        "\r\n",
        "  def call(self, inputs): # We define the forward pass computation in the call() method.\r\n",
        "    y = tf.matmul(inputs, self.W) + self.b\r\n",
        "    if self.activation is not None:\r\n",
        "      y = self.activation(y)\r\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76XFIEltu2Fy"
      },
      "source": [
        "Once instantiated, a layer like this can be used just like a function, taking as input a TensorFlow tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCsBwyRBs2Ld",
        "outputId": "f160dde3-9bd3-4efb-c5de-65e9feab53cd"
      },
      "source": [
        "my_dense = SimpleDense(units=32, activation=tf.nn.relu) # Instantiate our layer, defined above\r\n",
        "input_tensor = tf.ones(shape=(2, 784))  # Create some test inputs\r\n",
        "output_tensor = my_dense(input_tensor)  # Call the layer on the inputs, just like a function\r\n",
        "print(output_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9c6svkEww0x"
      },
      "source": [
        "#### Automatic shape inference: building layers on the fly\r\n",
        "\r\n",
        "Just like with LEGO bricks, you can only “clip” together layers that are compatible. The notion of layer compatibility here refers specifically to the fact that every layer will only accept input tensors of a certain shape and will return output tensors of a certain shape. Consider the following example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBS_FX9PvfJd"
      },
      "source": [
        "layer = layers.Dense(units=32, activation=\"relu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbifOUGCxQeZ"
      },
      "source": [
        "This layer will return a tensor where the first dimension has been transformed to be 32. It can only be connected to a downstream layer that expects 32-dimensional vectors as its input.\r\n",
        "\r\n",
        "When using Keras, you don’t have to worry about size compatibility most of the time, because the layers you add to your models are dynamically built to match the shape of the incoming layer. For instance, suppose you write the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qMHKNvJxK2R"
      },
      "source": [
        "model = models.Sequential([\r\n",
        "                           layers.Dense(32, activation=\"relu\"),\r\n",
        "                           layers.Dense(32)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0POhJbK0K_7"
      },
      "source": [
        "### From layers to models\r\n",
        "\r\n",
        "A deep-learning model is a graph of layers. In Keras, that’s the Model class. For now, you’ve only seen Sequential models (a subclass of Model), which are simple stack of layers, mapping a single input to a single output. But as you move forward, you’ll be exposed to a much broader variety of network topologies. Some common ones are:\r\n",
        "\r\n",
        "- Two-branch networks\r\n",
        "- Multihead networks\r\n",
        "- Residual connections\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3ChVNvDAIUT"
      },
      "source": [
        "### The \"compile\" step: configuring the learning process\r\n",
        "\r\n",
        "Once the model architecture is defined, you still have to choose three more things:\r\n",
        "\r\n",
        "**Loss function (objective function)**  — The quantity that will be minimized during training. It represents a measure of success for the task at hand.\r\n",
        "\r\n",
        "**Optimizer**  — Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).\r\n",
        "\r\n",
        "**Metrics** — The measures of success you want to monitor during training and validation, such as classification accuracy. Unlike the loss, training will not optimize directly for these metrics. As such, metrics don’t need to be differentiable.\r\n",
        "\r\n",
        "Once you’ve picked your loss, optimizer, and metrics, you can use the built-in compile() and fit() methods to start training your model.\r\n",
        "\r\n",
        "The compile() method configures the training process — you’ve already been introduced to it in your very first neural network example in chapter 2. It takes the argument optimizer, `loss, and metrics (a list):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcD0u-lpx1tC"
      },
      "source": [
        "model = keras.Sequential([keras.layers.Dense(units=1)]) # Define a linear classifier\r\n",
        "model.compile(optimizer=\"rmsprop\",  # Specify the optimizer by name: RMSprop (it’s case-insensitve)\r\n",
        "              loss=\"mean_squared_error\",  # Specify a list of metrics: in this case, only accuracy\r\n",
        "              metrics=[\"accuracy\"]) # Specify a list of metrics: in this case, only accuracy"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJYPzmUWByG1"
      },
      "source": [
        "In the above call to compile(), we passed the optimizer, loss, and metrics as strings (such as 'rmsprop'). These strings are actually shortcuts that get converted to Python objects. For instance, 'rmsprop' becomes keras.optimizers.RMSprop(). Importantly, it’s also possible to specify these arguments as object instances, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-G5hZZQBK0x"
      },
      "source": [
        "model.compile(optimizer=keras.optimizers.RMSprop(),\r\n",
        "              loss=keras.losses.MeanSquaredError(),\r\n",
        "              metrics=[keras.metrics.BinaryAccuracy()])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfzivhUsB-2a"
      },
      "source": [
        "This is useful if you want to pass your own custom losses or metrics, or if you want to further configure the objects you’re using — for instance, by passing a learning_rate argument to the optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MWw3LBJB0zh"
      },
      "source": [
        "# model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-4),\r\n",
        "#               loss=my_custom_loss,\r\n",
        "#               metrics=[my_custom_metric_1, my_custom_metric_2])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEg3S6P4CMP_"
      },
      "source": [
        "Keras offers a wide range of built-in options that is likely to include what you need:\r\n",
        "\r\n",
        "**Optimizers:**\r\n",
        "\r\n",
        "SGD() (with or without momentum)\r\n",
        "\r\n",
        "RMSprop()\r\n",
        "\r\n",
        "Adam()\r\n",
        "\r\n",
        "Adagrad()\r\n",
        "\r\n",
        "Etc.\r\n",
        "\r\n",
        "**Losses:**\r\n",
        "\r\n",
        "CategoricalCrossentropy()\r\n",
        "\r\n",
        "SparseCategoricalCrossentropy()\r\n",
        "\r\n",
        "BinaryCrossentropy()\r\n",
        "\r\n",
        "MeanSquaredError()\r\n",
        "\r\n",
        "KLDivergence()\r\n",
        "\r\n",
        "CosineSimilarity()\r\n",
        "\r\n",
        "Etc.\r\n",
        "\r\n",
        "**Metrics:**\r\n",
        "\r\n",
        "CategoricalAccuracy()\r\n",
        "\r\n",
        "SparseCategoricalAccuracy()\r\n",
        "\r\n",
        "BinaryAccuracy()\r\n",
        "\r\n",
        "AUC()\r\n",
        "\r\n",
        "Precision()\r\n",
        "\r\n",
        "Recall()\r\n",
        "\r\n",
        "Etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0FBhIubCtkK"
      },
      "source": [
        "### Picking a loss function\r\n",
        "\r\n",
        "Choosing the right loss function for the right problem is extremely important: your network will take any shortcut it can to minimize the loss; so if the objective doesn’t fully correlate with success for the task at hand, your network will end up doing things you may not have wanted. Imagine a stupid, omnipotent AI trained via SGD, with this poorly chosen objective function: “maximizing the average well-being of all humans alive.” To make its job easier, this AI might choose to kill all humans except a few and focus on the well-being of the remaining ones — because average well-being isn’t affected by how many humans are left. That might not be what you intended! Just remember that all neural networks you build will be just as ruthless in lowering their loss function — so choose the objective wisely, or you’ll have to face unintended side effects.\r\n",
        "\r\n",
        "Fortunately, when it comes to common problems such as classification, regression, and sequence prediction, there are simple guidelines you can follow to choose the correct loss. For instance, you’ll use binary crossentropy for a two-class classification problem, categorical crossentropy for a many-class classification problem, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkD7RSZhDZPA"
      },
      "source": [
        "### Understanding the \"fit\" method\r\n",
        "\r\n",
        "After compile() comes fit(). The fit method implements the training loop itself. Its key arguments are:\r\n",
        "\r\n",
        "The **data (inputs and targets)** to train on. It will typically be passed either in the form of NumPy arrays, of a TensorFlow Dataset object. \r\n",
        "\r\n",
        "The number of **epochs** to train for: how many times the training loop should iterate over the data passed.\r\n",
        "\r\n",
        "The **batch size** to use within each epoch of mini-batch gradient descent: the number of training examples considered to compute the gradients for one weight update step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HM_Jli1FFUg"
      },
      "source": [
        "\r\n",
        "num_samples_per_class = 1000\r\n",
        "negative_samples = np.random.multivariate_normal(\r\n",
        "    mean=[0, 3], cov=[[1, 0.5],[0.5, 1]], size=num_samples_per_class)\r\n",
        "positive_samples = np.random.multivariate_normal(\r\n",
        "    mean=[3, 0], cov=[[1, 0.5],[0.5, 1]], size=num_samples_per_class)\r\n",
        "inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)\r\n",
        "targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype='float32'),\r\n",
        "                     np.ones((num_samples_per_class, 1), dtype='float32')))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqFWSSD1CBJ_",
        "outputId": "0ba7d25d-5445-4d5f-9b23-054775b26d02"
      },
      "source": [
        "# Calling fit with NumPy data\r\n",
        "history = model.fit(\r\n",
        "  inputs, # The input examples, as a NumPy array\r\n",
        "  targets,  # The corresponding training targets, as a NumPy array\r\n",
        "  epochs=5, # The training loop will iterate over the data in batches of 128 examples.\r\n",
        "  batch_size=128  # The training loop will iterate over the data in batches of 128 examples.\r\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7.8464 - binary_accuracy: 0.9280\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - 0s 913us/step - loss: 7.5272 - binary_accuracy: 0.9260\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - 0s 991us/step - loss: 7.2576 - binary_accuracy: 0.9235\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - 0s 931us/step - loss: 6.9988 - binary_accuracy: 0.9220\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - 0s 933us/step - loss: 6.7458 - binary_accuracy: 0.9210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psRqU0RpD6nQ",
        "outputId": "5dcc19a6-3e3a-465a-da8a-6b0d292a12f1"
      },
      "source": [
        "history.history"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'binary_accuracy': [0.9279999732971191,\n",
              "  0.9259999990463257,\n",
              "  0.9235000014305115,\n",
              "  0.921999990940094,\n",
              "  0.9210000038146973],\n",
              " 'loss': [7.846403121948242,\n",
              "  7.52724027633667,\n",
              "  7.257632255554199,\n",
              "  6.998773097991943,\n",
              "  6.745824337005615]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz76T0orGCRp"
      },
      "source": [
        "### Monitoring loss & metrics on validation data\r\n",
        "\r\n",
        "To keep an eye on how the model does on new data, it’s standard practice to reserve a subset of the training data as “validation data”: you won’t be training the model on this data, but you will use it to compute a loss value and metrics value. You do this use the **validation_data** argument in **fit()**. Like the training data, the validation data could be passed as NumPy arrays or as a TensorFlow Dataset object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TcSze8mFnrL",
        "outputId": "e5c20749-0593-492e-9f40-58890ad700c5"
      },
      "source": [
        "#  Using the validation data argument\r\n",
        "model = keras.Sequential([keras.layers.Dense(1)])\r\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\r\n",
        "              loss=keras.losses.MeanSquaredError(),\r\n",
        "              metrics=[keras.metrics.BinaryAccuracy()])\r\n",
        "\r\n",
        "indices_permutation = np.random.permutation(len(inputs))  # To avoid having samples from only one class in the validation data,\r\n",
        "shuffled_inputs = inputs[indices_permutation]             #  shuffle the inputs and targets using a random indices permutation\r\n",
        "shuffled_targets = targets[indices_permutation]\r\n",
        "\r\n",
        "num_validation_samples = int(0.3 * len(inputs))           # Reserve 20% of the training inputs and targets for “validation” \r\n",
        "val_inputs = shuffled_inputs[-num_validation_samples:]    # (we’ll exclude these samples from training and reserve them \r\n",
        "val_targets = shuffled_targets[-num_validation_samples:]  # to compute the “validation loss” and metrics)\r\n",
        "training_inputs = shuffled_inputs[:num_validation_samples]\r\n",
        "training_targets = shuffled_targets[:num_validation_samples]\r\n",
        "\r\n",
        "history = model.fit(\r\n",
        "  training_inputs,  # Training data, used to update the weights of the model\r\n",
        "  training_targets,\r\n",
        "  epochs=5,\r\n",
        "  batch_size=16,\r\n",
        "  validation_data=(val_inputs, val_targets) # Validation data, used only to monitor the “validation loss” and metrics\r\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.8384 - binary_accuracy: 0.8217 - val_loss: 0.0658 - val_binary_accuracy: 0.9800\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0796 - binary_accuracy: 0.9500 - val_loss: 0.0293 - val_binary_accuracy: 0.9983\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0709 - binary_accuracy: 0.9517 - val_loss: 0.0315 - val_binary_accuracy: 0.9950\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0730 - binary_accuracy: 0.9583 - val_loss: 0.0564 - val_binary_accuracy: 0.9983\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0703 - binary_accuracy: 0.9550 - val_loss: 0.2277 - val_binary_accuracy: 0.8033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "linbtDkoIsc-"
      },
      "source": [
        "The value of the loss on the validation data is called the “validation loss”, to distinguish it from the “training loss”. Note that it’s essential to keep the training data and validation data strictly separate: the purpose of validation is to monitor whether what the model is learning is actually useful on new data. If any of the validation data has been seen by the model during training, your validation loss and metrics will be flawed.\r\n",
        "\r\n",
        "Note that if you want to compute the validation loss and metrics after training is complete, you can call the evaluate method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q45Yq-_NHyO4",
        "outputId": "46341ba2-28cc-4ca0-f1a0-cfb2e6f0c698"
      },
      "source": [
        "loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size=128)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 2ms/step - loss: 0.2277 - binary_accuracy: 0.8033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUxPJQIhI84k"
      },
      "source": [
        "evaluate() will iterate in batches (of size batch_size) over the data passed, and return a list of scalars, where the first entry is the validation loss and the following entries are the validation metrics. If the model has no metrics, only the validation loss is returned (rather than a list)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fukJ3-NVJI77"
      },
      "source": [
        "### Inference: using a model after training\r\n",
        "\r\n",
        "Once you’ve trained your model, you’re going to want to use it to make predictions on new data. This is called \"inference\". To do this, a naive approach would simply be to *call* the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jhLKC0KIv5o"
      },
      "source": [
        "predictions = model(new_inputs) # Takes a NumPy array or TensorFlow tensor and returns a TensorFlow tensor\r\n",
        "predictions = model.predict(new_inputs, batch_size=128) # Takes a NumPy array or a Dataset and returns a NumPy array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gJqB0KqK30q"
      },
      "source": [
        "For instance, if we use predict() on some of our validation data with the linear model we trained earlier, we get scalar scores between 0 and 1 — below 0.5 indicates that the model considers the corresponding point to belong to class 0, and above 0.5 indicates that the model considers the corresponding point to belong to class 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlZOZroOKZex",
        "outputId": "d3a847ab-6962-4c30-9dc2-093f6740122a"
      },
      "source": [
        " predictions = model.predict(val_inputs, batch_size=128)\r\n",
        " print(predictions[:10])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.3338068 ]\n",
            " [0.68316877]\n",
            " [0.45047593]\n",
            " [0.17697117]\n",
            " [1.401538  ]\n",
            " [0.4708088 ]\n",
            " [0.42412478]\n",
            " [1.0589097 ]\n",
            " [1.368237  ]\n",
            " [1.2308586 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}