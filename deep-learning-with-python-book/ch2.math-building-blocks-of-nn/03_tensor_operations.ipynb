{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03.tensor_operations.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu4cKLRywQi8"
      },
      "source": [
        "import numpy as np\r\n",
        "import time\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.datasets import mnist\r\n",
        "from tensorflow.keras import models, layers\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras import optimizers\r\n",
        "\r\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv4BSSr-watJ"
      },
      "source": [
        "Much as any computer program can be ultimately reduced to a small set of binary operations on binary inputs (AND, OR, NOR, and so on), all transformations learned by deep neural networks can be reduced to a handful of tensor operations applied to tensors of numeric data. For instance, it’s possible to add tensors, multiply tensors, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjt7jJFcxAYi"
      },
      "source": [
        "A Keras layer instance looks like this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqv6GPHZw2O5",
        "outputId": "f0471dc6-45f9-46b2-8838-4a2631be862f"
      },
      "source": [
        "Dense(512, activation='relu')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7f521d507e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lpxhx2xxCIY"
      },
      "source": [
        "This layer can be interpreted as a function, which takes as input a matrix and returns another matrix — a new representation for the input tensor. Specifically, the function is as follows (where W is a matrix and b is a vector, both attributes of the layer).\r\n",
        "\r\n",
        "We have three tensor operations here: a dot product (dot) between the input tensor and a tensor named W; an addition (+) between the resulting matrix and a vector b; and, finally, a relu operation. relu(x) is max(x, 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiMa3ZKdw4Lb"
      },
      "source": [
        "# output = relu(dot(W, input) + b)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXG2Evwdx1hl"
      },
      "source": [
        "### Element-wise operations\r\n",
        "\r\n",
        "The **relu** operation and **addition** are element-wise operations: operations that are applied independently to each entry in the tensors being considered. This means these operations are highly amenable to massively parallel implementations.\r\n",
        "\r\n",
        "If you want to write a naive Python implementation of an element-wise operation, you use a for loop, as in this naive implementation of an element-wise **relu** operation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pF5yd14xLNY"
      },
      "source": [
        "def naive_relu(x):\r\n",
        "    assert len(x.shape) == 2\r\n",
        "    x = x.copy()\r\n",
        "    for i in range(x.shape[0]):\r\n",
        "        for j in range(x.shape[1]):\r\n",
        "            x[i, j] = max(x[i, j], 0)\r\n",
        "    return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RUCQxjyyo7f"
      },
      "source": [
        "def naive_add(x, y):\r\n",
        "    assert len(x.shape) == 2\r\n",
        "    assert x.shape == y.shape\r\n",
        "    x = x.copy()\r\n",
        "    for i in range(x.shape[0]):\r\n",
        "        for j in range(x.shape[1]):\r\n",
        "            x[i, j] += y[i, j]\r\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZbG2GUqzWWz"
      },
      "source": [
        "On the same principle, you can do element-wise multiplication, subtraction, and so on.\r\n",
        "\r\n",
        "In practice, when dealing with NumPy arrays, these operations are available as well-optimized built-in NumPy functions, which themselves delegate the heavy lifting to a Basic Linear Algebra Subprograms (BLAS) implementation if you have one installed. BLAS are low-level, highly parallel, efficient tensor-manipulation routines that are typically implemented in Fortran or C.\r\n",
        "\r\n",
        "In NumPy, you can do the following element-wise operation, and it will be blazing fast"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_L27OyhzHyB"
      },
      "source": [
        "# z = x + y\r\n",
        "# z = np.maximum(z, 0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7tL3lQ3z1M9"
      },
      "source": [
        "Time the difference:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iqH4r4Rzpqk",
        "outputId": "4845cc0c-2aee-420d-856f-6affcd5a3f51"
      },
      "source": [
        "x = np.random.random((20, 100))\r\n",
        "y = np.random.random((20, 100))\r\n",
        "\r\n",
        "time_start = time.time()\r\n",
        "\r\n",
        "for _ in range(1000):\r\n",
        "  z = x + y\r\n",
        "  z = np.maximum(z, 0)\r\n",
        "\r\n",
        "duration = time.time() - time_start\r\n",
        "print(f\"Duration: {duration} sec\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Duration: 0.009510040283203125 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6vVHQ0Iz-2R",
        "outputId": "3b5ca4f0-9522-4102-e172-f7f58416d72a"
      },
      "source": [
        "time_start = time.time()\r\n",
        "for _ in range(1000):\r\n",
        "  z = naive_add(x, y)\r\n",
        "  z = naive_relu(z)\r\n",
        "\r\n",
        "duration = time.time() - time_start\r\n",
        "print(f\"Duration: {duration} sec\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Duration: 2.2906622886657715 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR-N4Wgl2vpH"
      },
      "source": [
        "### Broadcasting\r\n",
        "\r\n",
        "When possible, and if there’s no ambiguity, the smaller tensor will be broadcasted to match the shape of the larger tensor. Broadcasting consists of two steps:\r\n",
        "\r\n",
        "Axes (called broadcast axes) are added to the smaller tensor to match the ndim of the larger tensor.\r\n",
        "The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor.\r\n",
        "\r\n",
        "Example - Consider X with shape (32, 10) and y with shape (10,). First, we add an empty first axis to y, whose shape becomes (1, 10). Then, we repeat y 32 times alongside this new axis, so that we end up with a tensor Y with shape (32, 10), where Y[i, :] == y for i in range(0, 32). At this point, we can proceed to add X and Y, because they have the same shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zNWPtpM0nYP"
      },
      "source": [
        "def naive_add_matrix_and_vector(x, y):\r\n",
        "    assert len(x.shape) == 2\r\n",
        "    assert len(y.shape) == 1\r\n",
        "    assert x.shape[1] == y.shape[0]\r\n",
        "    x = x.copy()\r\n",
        "    for i in range(x.shape[0]):\r\n",
        "        for j in range(x.shape[1]):\r\n",
        "            x[i, j] += y[j]\r\n",
        "    return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa6p3c-0pfP"
      },
      "source": [
        "x = np.random.random((64, 3, 32, 10))\r\n",
        "y = np.random.random((32, 10))\r\n",
        "z = np.maximum(x, y)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAxN1wW756CM"
      },
      "source": [
        "### Tensor product\r\n",
        "The tensor product, or dot product (not to be confused with an element-wise product, the * operator) is one of the most common, most useful tensor operations.\r\n",
        "\r\n",
        "In NumPy, a tensor product is done using the np.dot function (because the mathematical notation for tensor product is usually a dot)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C52RvqVB40Xi"
      },
      "source": [
        "x = np.random.random((32,))\r\n",
        "y = np.random.random((32,))\r\n",
        "z = np.dot(x, y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iKgbBsq41Co",
        "outputId": "95a38245-7a07-4e76-8200-45079b1aa2d2"
      },
      "source": [
        "z"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.009199012317854"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ2fvdno6Y_k"
      },
      "source": [
        "# naive interpretation of two vectors\r\n",
        "def naive_vector_dot(x, y):\r\n",
        "    assert len(x.shape) == 1\r\n",
        "    assert len(y.shape) == 1\r\n",
        "    assert x.shape[0] == y.shape[0]\r\n",
        "    z = 0.\r\n",
        "    for i in range(x.shape[0]):\r\n",
        "        z += x[i] * y[i]\r\n",
        "    return z"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2LulnNZ6s72"
      },
      "source": [
        "zz = naive_vector_dot(x, y)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1Y4rlps60pP",
        "outputId": "cb21c7cf-a41e-45d7-c83a-ecc069f959d2"
      },
      "source": [
        "zz"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.009199012317853"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMnxiOSJ61Uw"
      },
      "source": [
        "# naive interpretation of matrix and vector\r\n",
        "def naive_matrix_vector_dot(x, y):\r\n",
        "    assert len(x.shape) == 2\r\n",
        "    assert len(y.shape) == 1\r\n",
        "    assert x.shape[1] == y.shape[0]\r\n",
        "    z = np.zeros(x.shape[0])\r\n",
        "    for i in range(x.shape[0]):\r\n",
        "        for j in range(x.shape[1]):\r\n",
        "            z[i] += x[i, j] * y[j]\r\n",
        "    return z"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhkZtucI7toV"
      },
      "source": [
        "As soon as one of the two tensors has an ndim greater than 1, dot is no longer symmetric, which is to say that dot(x, y) isn’t the same as dot(y, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAyTHRg77ztC"
      },
      "source": [
        "The most common applications may be the dot product between two matrices. You can take the dot product of two matrices x and y (dot(x, y)) if and only if x.shape[1] == y.shape[0] (mn nm). The result is a matrix with shape (x.shape[0], y.shape[1]), where the coefficients are the vector products between the rows of x and the columns of y. Here’s the naive implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNFAHurQ7cFc"
      },
      "source": [
        "def naive_matrix_dot(x, y):\r\n",
        "    assert len(x.shape) == 2\r\n",
        "    assert len(y.shape) == 2\r\n",
        "    assert x.shape[1] == y.shape[0]\r\n",
        "    z = np.zeros((x.shape[0], y.shape[1]))\r\n",
        "    for i in range(x.shape[0]):\r\n",
        "        for j in range(y.shape[1]):\r\n",
        "            row_x = x[i, :]\r\n",
        "            column_y = y[:, j]\r\n",
        "            z[i, j] = naive_vector_dot(row_x, column_y)\r\n",
        "    return z"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp694On28QUG"
      },
      "source": [
        "### Tensor reshaping\r\n",
        "\r\n",
        "Reshaping a tensor means rearranging its rows and columns to match a target shape. Naturally, the reshaped tensor has the same total number of coefficients as the initial tensor. Reshaping is best understood via simple examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZItmtoX8bj1",
        "outputId": "e2e6fe14-705b-44cd-8773-54d29d1aa60e"
      },
      "source": [
        "x = np.array([[0., 1.],\r\n",
        "              [2., 3.],\r\n",
        "              [4., 5.]])\r\n",
        "print(x.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSk5lElY80A5",
        "outputId": "67208760-0f91-4852-ed73-4a99811ae535"
      },
      "source": [
        "x = x.reshape((6, 1))\r\n",
        "x"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [1.],\n",
              "       [2.],\n",
              "       [3.],\n",
              "       [4.],\n",
              "       [5.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGDRcsi987bM",
        "outputId": "1a619c7f-3f34-4e62-9c66-b9c8ea19b6cd"
      },
      "source": [
        "x = x.reshape((2, 3))\r\n",
        "x"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 2.],\n",
              "       [3., 4., 5.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSH2ybaG9OLp"
      },
      "source": [
        "A special case of reshaping that’s commonly encountered is transposition. Transposing a matrix means exchanging its rows and its columns, so that x[i, :] becomes x[:, i]:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RJfe_Xk9A2a",
        "outputId": "5125ecee-e416-47f2-d867-e38f7c8a81ed"
      },
      "source": [
        "x = np.zeros((300, 20))\r\n",
        "print(x.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X5IwS-J9YoY",
        "outputId": "543988ce-5de3-4a94-d400-b126dad9a36b"
      },
      "source": [
        "x = np.transpose(x)\r\n",
        "print(x.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M0EDE4I9yWW"
      },
      "source": [
        "### Geometric interpretation of tensor operations\r\n",
        "\r\n",
        "Because the contents of the tensors manipulated by tensor operations can be interpreted as coordinates of points in some geometric space, all tensor operations have a geometric interpretation. For instance, let’s consider addition. We’ll start with the following vector:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQTGQoTXpwh8"
      },
      "source": [
        "### The engine of neural networks: gradient-based optimization\r\n",
        "\r\n",
        "Derivative of a tensor operation: the gradient\r\n",
        "\r\n",
        "Stochastic gradient descent\r\n",
        "\r\n",
        "Chaining derivatives: the Backpropagation algorithm\r\n",
        "\r\n",
        "The chain rule\r\n",
        "\r\n",
        "The Gradient Tape in TensorFlow - The API through which you can leverage TensorFlow’s powerful automatic differentiation capabilities is the GradientTape.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icqchhB598rc"
      },
      "source": [
        "x = tf.Variable(0.)\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  y = 2 * x + 3\r\n",
        "grad_of_y_wrt_x = tape.gradient(y, x)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anO3UsLt2akE",
        "outputId": "bf76db8c-7ce8-466b-ad85-162ae2454420"
      },
      "source": [
        "grad_of_y_wrt_x"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=2.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbW6nU9b1wzA"
      },
      "source": [
        "W = tf.Variable(tf.random.uniform((2, 2)))\r\n",
        "b = tf.Variable(tf.zeros((2,)))\r\n",
        "x = tf.random.uniform((2, 2))\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "    y = tf.matmul(W, x) + b\r\n",
        "grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvSW3bQN12MW",
        "outputId": "6d0cf20f-7596-4464-e40b-bce2831b3654"
      },
      "source": [
        "grad_of_y_wrt_W_and_b"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              " array([[0.9923886, 0.8159368],\n",
              "        [0.9923886, 0.8159368]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 2.], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyIKb9BX2YJK",
        "outputId": "42f778c7-24f6-4c0e-cc79-2e92d8e3be3b"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n",
        "train_images = train_images.reshape((60000, 28 * 28))\r\n",
        "train_images = train_images.astype('float32') / 255\r\n",
        "test_images = test_images.reshape((10000, 28 * 28))\r\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbJPqf0W3CM8"
      },
      "source": [
        "model = models.Sequential([\r\n",
        "  layers.Dense(512, activation='relu'),\r\n",
        "  layers.Dense(10, activation='softmax')\r\n",
        "])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDP7bJmF3UOU"
      },
      "source": [
        "model.compile(optimizer=\"rmsprop\",\r\n",
        "              loss=\"sparse_categorical_crossentropy\",\r\n",
        "              metrics=\"accuracy\")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaJChRYB4RPH",
        "outputId": "3680f852-a78e-44db-eb67-ec3238abe71c"
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.2586 - accuracy: 0.9244\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.1050 - accuracy: 0.9682\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0687 - accuracy: 0.9792\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0490 - accuracy: 0.9851\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0369 - accuracy: 0.9891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f51e1ce2e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5qO_LGY8fwh"
      },
      "source": [
        "Implementing from scratch in TensorFlow\r\n",
        "\r\n",
        "Let’s implement a simple Python class NaiveDense that creates two TensorFlow variables W and b, and exposes a call method that applies the above transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QDYOHX28Yy8"
      },
      "source": [
        "class NaiveDense:\r\n",
        "\r\n",
        "    def __init__(self, input_size, output_size, activation):\r\n",
        "        self.activation = activation\r\n",
        "\r\n",
        "        w_shape = (input_size, output_size) # create a matrix W of shape \"(input_size, output_size)\", initialized with random values\r\n",
        "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\r\n",
        "        self.W = tf.Variable(w_initial_value)\r\n",
        "\r\n",
        "        b_shape = (output_size,)  # create a vector b os shape (output_size, ), initialized with zeros\r\n",
        "        b_initial_value = tf.zeros(b_shape)\r\n",
        "        self.b = tf.Variable(b_initial_value)\r\n",
        "\r\n",
        "    def __call__(self, inputs): # apply the forward pass\r\n",
        "        return self.activation(tf.matmul(inputs, self.W) + self.b)\r\n",
        "\r\n",
        "    @property\r\n",
        "    def weights(self):  # convinience method for rettrieving the layer weights\r\n",
        "        return [self.W, self.b]"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw17s9p8_S5S"
      },
      "source": [
        "A simple Sequential class - create a NaiveSequential class to chain these layers. It wraps a list of layers, and exposes a call methods that simply call the underlying layers on the inputs, in order. It also features a weights property to easily keep track of the layers' parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2pqD8u09ZD1"
      },
      "source": [
        "class NaiveSequential:\r\n",
        "\r\n",
        "    def __init__(self, layers):\r\n",
        "        self.layers = layers\r\n",
        "\r\n",
        "    def __call__(self, inputs):\r\n",
        "        x = inputs\r\n",
        "        for layer in self.layers:\r\n",
        "           x = layer(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    @property\r\n",
        "    def weights(self):\r\n",
        "       weights = []\r\n",
        "       for layer in self.layers:\r\n",
        "           weights += layer.weights\r\n",
        "       return weights"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US0SVIaWArTx"
      },
      "source": [
        "Using this NaiveDense class and this NaiveSequential class, we can create a mock Keras model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFs8TbUa_n1u"
      },
      "source": [
        "model = NaiveSequential([\r\n",
        "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\r\n",
        "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\r\n",
        "])\r\n",
        "assert len(model.weights) == 4"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz1EC3khBejy"
      },
      "source": [
        "A batch generator\r\n",
        "\r\n",
        "Next, we need a way to iterate over the MNIST data in mini-batches. This is easy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o3PQNjWBNtt"
      },
      "source": [
        "class BatchGenerator:\r\n",
        "\r\n",
        "    def __init__(self, images, labels, batch_size=128):\r\n",
        "        self.index = 0\r\n",
        "        self.images = images\r\n",
        "        self.labels = labels\r\n",
        "        self.batch_size = batch_size\r\n",
        "\r\n",
        "    def next(self):\r\n",
        "        images = self.images[self.index : self.index + self.batch_size]\r\n",
        "        labels = self.labels[self.index : self.index + self.batch_size]\r\n",
        "        self.index += self.batch_size\r\n",
        "        return images, labels"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDkSG8IxCahJ"
      },
      "source": [
        "Running one training step\r\n",
        "\r\n",
        "The most difficult part of the process is the “training step”: updating the weights of the model after running it on one batch of data. We need to:\r\n",
        "\r\n",
        "1. Compute the predictions of the model for the images in the batch\r\n",
        "\r\n",
        "2. Compute the loss value for these predictions given the actual labels\r\n",
        "\r\n",
        "3. Compute the gradient of the loss with regard to the model’s weights\r\n",
        "\r\n",
        "4. Move the weights by a small amount in the direction opposite to the gradient\r\n",
        "\r\n",
        "To compute the gradient, we will use the TensorFlow GradientTape object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtZaJpahCud9"
      },
      "source": [
        "learning_rate = 1e-3\r\n",
        "\r\n",
        "def update_weights(gradients, weights):\r\n",
        "    for g, w in zip(gradients, weights):\r\n",
        "        w.assign_sub(w * learning_rate) # assign_sub is the equivalent of -= for TensorFlow variables"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU9NzX26BZG8"
      },
      "source": [
        "def one_training_step(model, images_batch, labels_batch):\r\n",
        "    with tf.GradientTape() as tape:   # run the \"forward pass\" (compute the model's predictions under the GradientTape scope)\r\n",
        "      predictions = model(images_batch)\r\n",
        "      per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(\r\n",
        "          labels_batch, predictions)\r\n",
        "      average_loss = tf.reduce_mean(per_sample_losses)\r\n",
        "    gradients = tape.gradient(average_loss, model.weights)  # compute the gradient of the loss with regard to the weights. The output gradients is a list where each entry corresponds to a weight from the models.weights list\r\n",
        "    update_weights(gradients, model.weights)  # update the weights using the gradients\r\n",
        "    return average_loss"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FohQjU0DDghq"
      },
      "source": [
        "In practice, you will almost never implement a weight update step like this by hand. Instead, you would use an Optimizer instance from Keras. Like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV6I-l0KDQYg"
      },
      "source": [
        "optimizer = optimizers.SGD(learning_rate=1e-3)\r\n",
        "\r\n",
        "def update_weights(gradients, weights):\r\n",
        "    optimizer.apply_gradients(zip(gradients, weights))"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5EwcFnIDwxU"
      },
      "source": [
        "The full training loop\r\n",
        "\r\n",
        "An epoch of training simply consists of the repetition of the training step for each batch in the training data, and the full training loop is simply the repetition of one epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSwdcBk-Djzr"
      },
      "source": [
        "def fit(model, images, labels, epochs, batch_size=128):\r\n",
        "    for epoch_counter in range(epochs):\r\n",
        "      print('Epoch %d' % epoch_counter)\r\n",
        "      batch_generator = BatchGenerator(images, labels)\r\n",
        "      for batch_counter in range(len(images) // batch_size):\r\n",
        "          images_batch, labels_batch = batch_generator.next()\r\n",
        "          loss = one_training_step(model, images_batch, labels_batch)\r\n",
        "          if batch_counter % 100 == 0:\r\n",
        "              print('loss at batch %d: %.2f' % (batch_counter, loss))"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z47YIkBoD-Qf",
        "outputId": "12cece31-efbc-4438-f6ef-add60318aef5"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\r\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n",
        "\r\n",
        "train_images = train_images.reshape((60000, 28 * 28))\r\n",
        "train_images = train_images.astype('float32') / 255\r\n",
        "test_images = test_images.reshape((10000, 28 * 28))\r\n",
        "test_images = test_images.astype('float32') / 255\r\n",
        "\r\n",
        "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "loss at batch 0: 6.83\n",
            "loss at batch 100: 2.24\n",
            "loss at batch 200: 2.21\n",
            "loss at batch 300: 2.12\n",
            "loss at batch 400: 2.22\n",
            "Epoch 1\n",
            "loss at batch 0: 1.93\n",
            "loss at batch 100: 1.89\n",
            "loss at batch 200: 1.84\n",
            "loss at batch 300: 1.75\n",
            "loss at batch 400: 1.84\n",
            "Epoch 2\n",
            "loss at batch 0: 1.61\n",
            "loss at batch 100: 1.59\n",
            "loss at batch 200: 1.52\n",
            "loss at batch 300: 1.46\n",
            "loss at batch 400: 1.53\n",
            "Epoch 3\n",
            "loss at batch 0: 1.33\n",
            "loss at batch 100: 1.35\n",
            "loss at batch 200: 1.26\n",
            "loss at batch 300: 1.24\n",
            "loss at batch 400: 1.29\n",
            "Epoch 4\n",
            "loss at batch 0: 1.12\n",
            "loss at batch 100: 1.16\n",
            "loss at batch 200: 1.05\n",
            "loss at batch 300: 1.07\n",
            "loss at batch 400: 1.12\n",
            "Epoch 5\n",
            "loss at batch 0: 0.97\n",
            "loss at batch 100: 1.02\n",
            "loss at batch 200: 0.90\n",
            "loss at batch 300: 0.95\n",
            "loss at batch 400: 1.00\n",
            "Epoch 6\n",
            "loss at batch 0: 0.86\n",
            "loss at batch 100: 0.91\n",
            "loss at batch 200: 0.80\n",
            "loss at batch 300: 0.85\n",
            "loss at batch 400: 0.91\n",
            "Epoch 7\n",
            "loss at batch 0: 0.77\n",
            "loss at batch 100: 0.83\n",
            "loss at batch 200: 0.72\n",
            "loss at batch 300: 0.78\n",
            "loss at batch 400: 0.84\n",
            "Epoch 8\n",
            "loss at batch 0: 0.71\n",
            "loss at batch 100: 0.76\n",
            "loss at batch 200: 0.65\n",
            "loss at batch 300: 0.72\n",
            "loss at batch 400: 0.79\n",
            "Epoch 9\n",
            "loss at batch 0: 0.66\n",
            "loss at batch 100: 0.70\n",
            "loss at batch 200: 0.60\n",
            "loss at batch 300: 0.67\n",
            "loss at batch 400: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq5qKjFKGkWQ"
      },
      "source": [
        " Evaluating the model\r\n",
        "\r\n",
        " We can evaluate the model by taking the argmax of its predictions over the test images, and comparing it to the expected labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQlOiDz9EOSR",
        "outputId": "a3a8679b-c028-438a-b94e-102a07b8f1d9"
      },
      "source": [
        "predictions = model(test_images)\r\n",
        "predictions = predictions.numpy() # calling .numpy() to a TensorFlow tensor converts it to a NumPy tensor\r\n",
        "predicted_labels = np.argmax(predictions, axis=1)\r\n",
        "matches = predicted_labels == test_labels\r\n",
        "# print('accuracy: %.2f' % matches.average())\r\n",
        "print(f\"Accuracy: {np.average(matches)}\")"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8318\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}